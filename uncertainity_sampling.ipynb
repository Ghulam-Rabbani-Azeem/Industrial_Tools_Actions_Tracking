{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "from data_preprocessing import load_combined_data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761338db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # disables optimizations for reproducibility\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5946a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load preprocessed data\n",
    "combined_data, combined_labels = load_combined_data(data_path='ES_Down_combined_data.csv', labels_path='ES_Down_combined_labels.csv',downsample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map labels from original multi-class to binary\n",
    "def map_labels(original_labels,classification_type='binary'):\n",
    "    if classification_type == 'binary':\n",
    "        label_map = {2: 0, 3: 1}  # 0: tightening, 1: untightening\n",
    "        num_classes = 2\n",
    "    return np.array([label_map[label] if label in label_map else -1 for label in original_labels]),num_classes\n",
    "\n",
    "# Filter and preprocess the data\n",
    "binary_labels,num_of_classes = map_labels(combined_labels,classification_type='binary')\n",
    "valid_indices = binary_labels != -1\n",
    "X = combined_data[valid_indices, :, 1:]  # exclude timestamps\n",
    "y = binary_labels[valid_indices]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y) # can be removed as our code uses binary labels directly\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.LongTensor(y_encoded)\n",
    "\n",
    "# ---- 1. Split into train/test ----\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, stratify=y_tensor)\n",
    "\n",
    "\n",
    "# ---- 2. Split train into labeled and unlabeled pools ----\n",
    "initial_ratio = 0.1\n",
    "train_indices = np.arange(len(X_train))\n",
    "y_train_np = y_train.numpy()\n",
    "\n",
    "initial_indices, unlabeled_indices = train_test_split(\n",
    "    train_indices, train_size=initial_ratio, stratify=y_train_np)\n",
    "\n",
    "# ---- 3. Track Global Index (Optional but useful) ----\n",
    "initial_indices_global = train_indices[initial_indices]\n",
    "unlabeled_indices_global = train_indices[unlabeled_indices]\n",
    "\n",
    "# ---- 4. Create pools ----\n",
    "X_labeled = X_train[initial_indices]\n",
    "y_labeled = y_train[initial_indices]\n",
    "\n",
    "X_unlabeled_pool = X_train[unlabeled_indices]\n",
    "y_unlabeled_pool = y_train[unlabeled_indices]\n",
    "\n",
    "print(f\"Initial labeled pool: {len(X_labeled)} samples\")\n",
    "print(f\"Initial unlabeled pool: {len(X_unlabeled_pool)} samples\\n\")\n",
    "\n",
    "# ---- 5. Normalize based on labeled training data ----\n",
    "scalers = {}\n",
    "for i in range(X_labeled.shape[2]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    X_labeled[:, :, i] = torch.FloatTensor(scalers[i].fit_transform(X_labeled[:, :, i]))\n",
    "    X_unlabeled_pool[:, :, i] = torch.FloatTensor(scalers[i].transform(X_unlabeled_pool[:, :, i]))\n",
    "    X_test[:, :, i] = torch.FloatTensor(scalers[i].transform(X_test[:, :, i]))\n",
    "\n",
    "# ---- 6. Create DataLoaders ----\n",
    "batch_size = 64 # [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "train_loader = DataLoader(TensorDataset(X_labeled, y_labeled), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ---- 7. Label Distribution Printout ---\n",
    "def print_label_distribution(labels, label_encoder, name=''):\n",
    "    unique, counts = torch.unique(labels, return_counts=True)\n",
    "    print(f\"{name} label distribution:\")\n",
    "    for label, count in zip(unique.tolist(), counts.tolist()):\n",
    "        print(f\"  Label {label_encoder.inverse_transform([label])[0]}: {count} samples\")\n",
    "    return unique, counts\n",
    "\n",
    "unique_train, counts_train = print_label_distribution(y_labeled, label_encoder, name='Labeled train set')\n",
    "unique_test, counts_test = print_label_distribution(y_test, label_encoder, name='Test set')\n",
    "\n",
    "# ---- 8. Display the selected indices\n",
    "def display_selected_indices(selected_indices,  title=\"Selected Samples\"):\n",
    "    \"\"\"\n",
    "    Prints the names of selected samples given their indices.\n",
    "\n",
    "    Args:\n",
    "        selected_indices (array-like): Indices of selected samples.\n",
    "        sample_names (list): List of sample names (indexed same as dataset).\n",
    "        title (str): Optional title for the print output.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title} ({len(selected_indices)} samples)\")\n",
    "    print(f'Selected indices: {selected_indices}')\n",
    "\n",
    "display_selected_indices(initial_indices_global, title=\"Initial Labeled Indices\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pools(selected_indices):\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the labeled and unlabeled pools based on selected indices.\n",
    "    Args:\n",
    "        selected_indices (array-like): Indices of samples to move from unlabeled to labeled.\n",
    "    Returns:\n",
    "        DataLoader: Updated train DataLoader with new labeled data.\n",
    "    \"\"\"\n",
    "\n",
    "    global X_labeled, y_labeled, X_unlabeled_pool, y_unlabeled_pool, train_loader\n",
    "\n",
    "    # Move selected to labeled\n",
    "    selected_X = X_unlabeled_pool[selected_indices]\n",
    "    selected_y = y_unlabeled_pool[selected_indices]\n",
    "\n",
    "    X_labeled = torch.cat([X_labeled, selected_X], dim=0)\n",
    "    y_labeled = torch.cat([y_labeled, selected_y], dim=0)\n",
    "\n",
    "    # Remove from unlabeled\n",
    "    mask = torch.ones(len(X_unlabeled_pool), dtype=bool)\n",
    "    mask[selected_indices] = False\n",
    "    X_unlabeled_pool = X_unlabeled_pool[mask]\n",
    "    y_unlabeled_pool = y_unlabeled_pool[mask]\n",
    "\n",
    "    # Rebuild DataLoader\n",
    "    train_loader = DataLoader(TensorDataset(X_labeled, y_labeled), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Updated labeled pool: {len(X_labeled)} samples\")\n",
    "    print(f\"Remaining unlabeled pool: {len(X_unlabeled_pool)} samples\")\n",
    "\n",
    "    return train_loader # no need to return DataLoader, as we can access it globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def uncertainty_sampling(model, unlabeled_data, strategy, k, us_batch_size):\n",
    "    \"\"\"\n",
    "    Perform uncertainty sampling to select the most uncertain samples from the unlabeled data.\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to use for uncertainty estimation.\n",
    "        unlabeled_data (Tensor): The unlabeled data from which to select samples.\n",
    "        strategy (str): The uncertainty sampling strategy ('lc', 'sm', or 'entropy').\n",
    "        k (int): The number of samples to select.\n",
    "        us_batch_size (int): Batch size for processing the unlabeled data.\n",
    "    Returns:\n",
    "        Tuple: Indices of the selected samples and their corresponding probabilities.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(unlabeled_data), us_batch_size):\n",
    "            batch = unlabeled_data[i:i + us_batch_size]\n",
    "            logits = model(batch)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            all_probs.append(probs)\n",
    "\n",
    "            if strategy == 'lc':\n",
    "                max_probs, _ = probs.max(dim=1)\n",
    "                scores = 1 - max_probs\n",
    "\n",
    "            elif strategy == 'sm':\n",
    "                sorted_probs, _ = probs.sort(descending=True, dim=1)\n",
    "                scores = -(sorted_probs[:, 0] - sorted_probs[:, 1])\n",
    "\n",
    "            elif strategy == 'entropy':\n",
    "                scores = -torch.sum(probs * torch.log(probs + 1e-12), dim=1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Strategy must be 'lc', 'sm', or 'entropy'\")\n",
    "\n",
    "            all_scores.append(scores)\n",
    "\n",
    "    all_scores = torch.cat(all_scores)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "\n",
    "    # Get indices of top-k uncertain samples\n",
    "    topk = torch.topk(all_scores, k=k)\n",
    "    return topk.indices, all_probs[topk.indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced LSTM model with more hidden layers\n",
    "class EnhancedToolLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):\n",
    "        super(EnhancedToolLSTM, self).__init__()\n",
    "        # Three LSTM layers\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm3 = nn.LSTM(hidden_size2, hidden_size3, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        # Expanded fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size3, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout3(out)\n",
    "        out = out[:, -1, :]  # Last time step\n",
    "\n",
    "        out = self.leaky_relu(self.bn1(self.fc1(out)))\n",
    "        out = self.leaky_relu(self.bn2(self.fc2(out)))\n",
    "        out = self.leaky_relu(self.bn3(self.fc3(out)))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.relu(self.fc5(out))\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "# Dynamic Focal Loss with adjustable class weights\n",
    "class DynamicFocalLoss(nn.Module):\n",
    "    def __init__(self, initial_weights=None, gamma=2.0, adjustment_rate=0.1, max_adjustment=0.5):\n",
    "        super(DynamicFocalLoss, self).__init__()\n",
    "        self.initial_weights = initial_weights\n",
    "        self.weights = initial_weights.clone() if initial_weights is not None else None\n",
    "        self.gamma = gamma\n",
    "        self.adjustment_rate = adjustment_rate\n",
    "        self.max_adjustment = max_adjustment\n",
    "        self.class_performance = None\n",
    "\n",
    "    def update_weights(self, class_performance):\n",
    "        \"\"\"Adjust weights based on class performance (accuracy per class)\"\"\"\n",
    "        if self.weights is None:\n",
    "            return\n",
    "            \n",
    "        # Calculate adjustment factor (boost weights for poorly performing classes)\n",
    "        adjustment_factors = 1.0 - class_performance\n",
    "        adjustment_factors = torch.clamp(adjustment_factors * self.adjustment_rate, \n",
    "                                      -self.max_adjustment, self.max_adjustment)\n",
    "        \n",
    "        # Apply adjustment\n",
    "        new_weights = self.weights * (1 + adjustment_factors)\n",
    "        new_weights = new_weights / new_weights.sum()  # Renormalize\n",
    "        \n",
    "        # Store for next epoch\n",
    "        self.weights = new_weights\n",
    "        self.class_performance = class_performance\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        logpt = nn.functional.log_softmax(inputs, dim=1)\n",
    "        pt = torch.exp(logpt)\n",
    "        logpt = logpt.gather(1, targets.view(-1, 1))\n",
    "        pt = pt.gather(1, targets.view(-1, 1))\n",
    "\n",
    "        if self.weights is not None:\n",
    "            weights_t = self.weights.gather(0, targets)\n",
    "            logpt = logpt * weights_t.view(-1, 1)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        return loss.mean()\n",
    "    \n",
    "\n",
    "def train_test_lstm():\n",
    "    \"\"\"Train and test the LSTM model with dynamic focal loss and uncertainty sampling.\"\"\"\n",
    "      \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EnhancedToolLSTM(input_size=X.shape[2], \n",
    "                            hidden_size1=128, \n",
    "                            hidden_size2=64, \n",
    "                            hidden_size3=32, \n",
    "                            num_classes=num_of_classes).to(device)\n",
    "\n",
    "    # Initialize class weights (inverse of class frequencies)\n",
    "    initial_weights = 1. / counts_train.float()\n",
    "    initial_weights = initial_weights / initial_weights.sum()\n",
    "    initial_weights = initial_weights.to(device)\n",
    "\n",
    "    # Dynamic loss function\n",
    "    criterion = DynamicFocalLoss(initial_weights=initial_weights, \n",
    "                            gamma=2.0, \n",
    "                            adjustment_rate=0.1,\n",
    "                            max_adjustment=0.3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    # Initialize containers for tracking metrics\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    class_performance_history = []  # To track per-class accuracy\n",
    "\n",
    "    # Training loop with test evaluation each epoch\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        running_loss = 0.0\n",
    "        class_correct = torch.zeros_like(initial_weights)\n",
    "        class_total = torch.zeros_like(initial_weights)\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Track per-class accuracy\n",
    "            for l in torch.unique(labels):\n",
    "                mask = labels == l\n",
    "                class_correct[l] += (predicted[mask] == labels[mask]).sum().item()\n",
    "                class_total[l] += mask.sum().item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_accuracies.append(correct / total)\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        class_performance = class_correct / class_total.clamp(min=1)  # Avoid division by zero\n",
    "        class_performance_history.append(class_performance.cpu().numpy())\n",
    "        \n",
    "        # Update class weights based on performance\n",
    "        criterion.update_weights(class_performance)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct, test_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        test_accuracies.append(test_correct / test_total)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(test_losses[-1])\n",
    "\n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "        print(f\"  Train Acc: {train_accuracies[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}\")\n",
    "        print(f\"  Class Performance: {dict(zip([label_encoder.inverse_transform([i])[0] for i in range(len(class_performance))], [f'{acc:.4f}' for acc in class_performance]))}\")\n",
    "        print(f\"  Current Weights: {dict(zip([label_encoder.inverse_transform([i])[0] for i in range(len(criterion.weights))], [f'{w:.4f}' for w in criterion.weights]))}\")\n",
    "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"\\nFinal Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Final F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    TT = cm[0, 0]\n",
    "    TF = cm[0, 1]\n",
    "    FT = cm[1, 0]\n",
    "    FF = cm[1, 1]\n",
    "    print(f\"TT (True tightening): {TT}\")\n",
    "    print(f\"TF (Tightening predicted as untightening): {TF}\")\n",
    "    print(f\"FT (Untightening predicted as tightening): {FT}\")\n",
    "    print(f\"FF (True untightening): {FF}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['tightening', 'untightening']))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['tightening', 'untightening'],\n",
    "                yticklabels=['tightening', 'untightening'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Class Performance\n",
    "    plt.subplot(2, 2, 4)\n",
    "    class_perf_array = np.array(class_performance_history)\n",
    "    for i in range(class_perf_array.shape[1]):\n",
    "        plt.plot(class_perf_array[:, i], label=f'{label_encoder.inverse_transform([i])[0]} accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Class Accuracy')\n",
    "    plt.title('Class-wise Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, train_accuracies[-1], test_accuracies[-1], train_losses[-1], test_losses[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------- [START] Main Active Learning Loop -----------------------------------------------------------\n",
    "\n",
    "# Active Learning Parameters\n",
    "num_queries = 5            # How many active learning rounds\n",
    "samples_per_query = 20     # How many samples to add per round\n",
    "uncertainity_strategy = 'entropy'  # 'lc', 'sm', or 'entropy'\n",
    "uncertainity_strategy_batch_size = 8  # Batch size for uncertainty sampling\n",
    "\n",
    "strategy_map = {\n",
    "    'lc': 'Least Confidence',\n",
    "    'sm': 'Small Margin',\n",
    "    'entropy': 'Entropy'\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results_summary = pd.DataFrame(columns=[\n",
    "    \"Round\", \"AL_Method\", \"Training_Batch_Size\",\"Uncertainity_Strategy_Batch_Size\", \"Num_Samples\",\"Selected_Indices\", \"Train_Acc\", \"Test_Acc\", \"Train_Loss\", \"Test_Loss\"\n",
    "])\n",
    "\n",
    "\n",
    "for num in range(num_queries):\n",
    "    print(f\"\\n--- Active Learning Round {num + 1} ---\")\n",
    "\n",
    "\n",
    "    # Train model and collect metrics\n",
    "    model, train_acc, test_acc, train_loss, test_loss = train_test_lstm()\n",
    "\n",
    "    # Build row as a dict\n",
    "    row_dict = {\n",
    "        \"Round\": num + 1,\n",
    "        \"AL_Method\": strategy_map.get(uncertainity_strategy, 'Unknown'),\n",
    "        \"Training_Batch_Size\": batch_size,\n",
    "        \"Uncertainity_Strategy_Batch_Size\": uncertainity_strategy_batch_size,\n",
    "        \"Num_Samples\": len(y_labeled),\n",
    "        \"Selected_Indices\": str(initial_indices_global.tolist()) if num == 0 else str(selected_indices.tolist()),\n",
    "        \"Train_Acc\": round(train_acc * 100, 2),\n",
    "        \"Test_Acc\": round(test_acc * 100, 2),\n",
    "        \"Train_Loss\": round(train_loss, 4),\n",
    "        \"Test_Loss\": round(test_loss, 4)\n",
    "    }\n",
    "\n",
    "    # Append row using loc\n",
    "    results_summary.loc[len(results_summary)] = row_dict\n",
    "\n",
    "    # Select most uncertain samples from unlabeled pool\n",
    "    selected_indices, selected_probs = uncertainty_sampling(\n",
    "    model, X_unlabeled_pool, strategy= uncertainity_strategy, k=samples_per_query, us_batch_size=uncertainity_strategy_batch_size\n",
    ")\n",
    "    # Display selected indices\n",
    "    display_selected_indices(selected_indices, title=f\"Selected Labeled Indices in round {num + 1}\")\n",
    "\n",
    "    # Update pools and train_loader\n",
    "    train_loader = update_pools(selected_indices)\n",
    "\n",
    "    print(\"Training samples count for weights initialization:\", counts_train)\n",
    "    \n",
    "    # Optional: Check new label distribution\n",
    "    unique_train, counts_train = print_label_distribution(y_labeled, label_encoder, name='Updated Labeled Pool')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Summary of Active Learning Rounds ---\")\n",
    "print(results_summary)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"active_learning_results.csv\"\n",
    "if not os.path.exists(csv_path):\n",
    "    results_summary.to_csv(csv_path, index=False)\n",
    "else:\n",
    "    results_summary.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "print(f\"\\nResults saved to {csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tool-tracking_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
